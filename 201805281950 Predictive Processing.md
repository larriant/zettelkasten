---
title: 201805281949 Predictive Processing
date: 2018-05-28 19:49
tags: #psychadelics #neuroscience
---

Free energy Principle. 

The brain is an inference machine that actively predicts and explains its sensations.

There is a probabilistic model that can generate predicitons, against which sensory samples are tested to update beliegs about their causes.

The brain works to minimize its own prediction error.

Higher-level areas of the nervous system generate top-down synpatic 'predictions' aimed at matching the expected bottom-up synaptic activity at lower-level areas. All the way down to 'input' activity at sense organs.

Higher-level areas attempt to 'explain' the states of levels below via synaptic attempts to inhibit lower-level activity.
i.e. high-level areas tell lower levels to 'shut up.'


But they won't 'shut up' until they receive top-down feedback signals that adequately fit (explain) the bottom-up evidence signals.

Mismatches between synpatic 'expectation' and synaptic 'evidence' generate 'prediction error signals' which carry the news by propagating 'surprise' upward.

This recurrent neural processing scheme approximates Bayesian Inference.

Crucially the sets of possible causes must be narrowed in order for the system to settle on an explanation.

Prior constraints which allow the system to narrow the hypothesis space are known as 'inductive biases' or priors in Bayesian statistics.

Certain priors in the hierarchy, known as 'hyperpriors' are more abstract and allow the system to 'rule out' large swaths of possibilities.

"[A] fruitful way of looking at the human brain, therefore, is as a system which, even in ordinary waking states, constantly hallucinates at the world, as a system that constantly lets its internal autonomous simulational dynamics collide with the ongoing flow of sensory input, vigorously dreaming at the world and thereby generating the content of phenomenal experience”

Normal perception as a kind of 'controlled hallucination'.

Psychadelic drugs essentially cause perception to be _less controlled_ hallucination.

The idea is that psychadelic drugs perturb the (learned and innate) prior constraints on internal generative models.

But does this come into conflict with Huxley's idea that psychadelics reveal __more__ of the world?

Active inference shows how internal models do not merely generate top-down (inference) signals but also shape the sampling and accumulation of bottom up sensory signals.

An intuitive example would be feeling our way in darkness: we anticipate what we might touch next and then try to confirm those expectations.

Psychedelics manifest mind by perturbing prior constraints on internal generative models, thereby expanding the possibilities in our inner world of feelings, thoughts, and mental imagery. 

Importantly, this could also manifest normally ignored aspects of world by altering active inference, which would in effect expand the sampling of sensory data to include samples that are normally routinely ‘explained away.’ 